{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "446dd04e-debb-416d-9615-f2ca17c53de9",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34be8f0-2fbf-4c8c-b3cc-0b6fe0d13a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "# Packages you will need in the Deployment Stage\n",
    "\n",
    "import lab_utils\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import json\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ee7a0-b777-4722-9452-502a6ad6b4c4",
   "metadata": {},
   "source": [
    "Install tensorflow serving Model server in your system by following instructions on the below link. <br>\n",
    "[Tensorflow Serving installation](https://www.tensorflow.org/tfx/serving/setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ab11bb-1dc9-4988-9e1e-a5f9f38afdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment the lines below if you ran this section before and want to DELETE all models in the serving directory\n",
    "# SERVING_DIR = f'{os.getcwd()}/serving'\n",
    "# os.environ[\"SERVING_DIR\"] = SERVING_DIR\n",
    "# os.system('find $SERVING_DIR -maxdepth 1 -mindepth 1 -type d -exec rm -rf {} \\;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4fdc7db-a13b-4bd8-8b7d-5974284c5868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVING_DIR: /home/jovyan/work/serving\n",
      "os.environ[\"SERVING_DIR\"]: /home/jovyan/work/serving\n"
     ]
    }
   ],
   "source": [
    "## Setting the environment variable for serving directory\n",
    "SERVING_DIR = f'{os.getcwd()}/serving'\n",
    "os.environ['SERVING_DIR'] = SERVING_DIR\n",
    "\n",
    "print(f'SERVING_DIR: {SERVING_DIR}')\n",
    "print(f'os.environ[\"SERVING_DIR\"]: {os.environ[\"SERVING_DIR\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3f38eee-c17d-4fed-adca-2f6f2afd73bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/serving/1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now copy the model in store2 into the serving directory under a folder named 1.\n",
    "os.makedirs(f'{SERVING_DIR}/1', exist_ok=True)\n",
    "shutil.copytree('./store2/model/', f'{SERVING_DIR}/1', dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f5bfc-5dea-4fb3-a159-a7941d12c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Copy and uncomment this on a new cell.\n",
    "\n",
    "# command = (\n",
    "#     'docker run -p 8501:8501 --mount type=bind,source=\"${SERVING_DIR}\",target=/models/newsapp_model '+\n",
    "#     '-e MODEL_NAME=newsapp_model --name=tensorflow-serving -t tensorflow/serving &'\n",
    "# )\n",
    "\n",
    "# os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08de5fd9-3c33-40d2-9a14-01abbd0a793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg \n",
    "nohup tensorflow_model_server \\\n",
    "  --rest_api_port=8501 \\\n",
    "  --model_name=newsapp_model \\\n",
    "  --model_base_path=\"${SERVING_DIR}\" > ./serving/server.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ae8f5-e1c6-415d-b500-2dec152ffba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = (\n",
    "    'curl -d \\'{\"instances\": [[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]]}\\' \\\\' +\n",
    "    '-X POST http://localhost:8501/v1/models/newsapp_model:predict'\n",
    ")\n",
    "\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da90a0a-d966-4ad3-a061-03a30abf076d",
   "metadata": {},
   "source": [
    "You can also send requests programmatically using the code snippet below. This example demonstrates how to load the title preprocessor to transform raw strings into a format the model can process. While this approach will work, there might be a potential issue. Can you identify the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ecd748-9c6e-4c21-8980-f433993cd933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used by the team on their prototype\n",
    "MAX_LENGTH = 20\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Instantiate a layer for text preprocessing\n",
    "title_preprocessor = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "# Load the vocabulary file\n",
    "title_preprocessor.load_assets('./E2/vocab')\n",
    "\n",
    "# Sample input\n",
    "sample_input = 'Sample title'\n",
    "\n",
    "# Preprocess the string\n",
    "sample_input_ds = title_preprocessor(sample_input)\n",
    "\n",
    "# Add a batch dimension\n",
    "sample_input_ds = tf.expand_dims(sample_input_ds, axis=0)\n",
    "\n",
    "# Compose the data\n",
    "data = json.dumps({\"instances\": sample_input_ds.numpy().tolist()})\n",
    "\n",
    "# Define the headers\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "\n",
    "# Send the request\n",
    "json_response = requests.post('http://localhost:8501/v1/models/newsapp_model:predict', data=data, headers=headers)\n",
    "\n",
    "# Get the predictions\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f3ef8a-5994-478a-9562-5301727e8420",
   "metadata": {},
   "source": [
    "When serving models, it's crucial to ensure that data transformations are consistent with those used during training. The code above risks errors in loading the vocabulary, especially if youâ€™ve edited it for multiple experiments. For example, you might accidentally load the wrong vocabulary file (e.g., from Experiment 3 instead of Experiment 2), leading to unreliable predictions because the titles are being preprocessed differently.\n",
    "<br><br>\n",
    "To prevent this, frameworks and tools often provide mechanisms to link preprocessing steps directly to the model. In TensorFlow, you can attach the title preprocessor to the model's input. This ensures the correct vocabulary is always associated with the model, allowing you to pass strings directly without worrying about mismatches. You'll implement this approach below.\n",
    "<br>\n",
    "Note that this is usually done after you've trained the model and are ready to deploy. If you attach it before that, it will likely slow down the training especially if you're using GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ca868-236d-48ca-b02d-b5a1e890df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working folder for the experiment\n",
    "BASE_DIR = './store2'\n",
    "\n",
    "# get the subdirectories that contain the experiment files\n",
    "data_dir, model_dir, vocab_dir = lab_utils.set_experiment_dirs(BASE_DIR)\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_dir)\n",
    "\n",
    "# Instantiate a layer for text preprocessing\n",
    "title_preprocessor = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "# Load the vocabulary file\n",
    "title_preprocessor.load_assets(vocab_dir)\n",
    "\n",
    "# Attach the preprocessing layer to the trained model\n",
    "model_with_preprocessor = tf.keras.Sequential([\n",
    "    title_preprocessor,\n",
    "    model\n",
    "])\n",
    "\n",
    "# String input\n",
    "sample_input = \"Sample Title\"\n",
    "\n",
    "# Feed the string input directly to the model\n",
    "model_with_preprocessor.predict([sample_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ebccff-22c6-4fef-8b9e-2fe554112994",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "can now deploy this version of the model as well. Aside from model.save(), you can also use model.export() \n",
    "which saves a lightweight version of the model dedicated for serving. \n",
    "Run the cell below to save this in the serving directory under the 2 folder.\n",
    "'''\n",
    "model_with_preprocessor.export(f'{SERVING_DIR}/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fda84-8d27-456c-9891-fc3a2acd9907",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tensorflow Serving will detect this latest model and you can see below that you can just send string inputs to the model. \n",
    "##If you get an error, please wait one minute before running the cell again.\n",
    "\n",
    "data = json.dumps({\"instances\": [\"sample title\"]})\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post('http://localhost:8501/v1/models/newsapp_model:predict', data=data, headers=headers)\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb066f3-7f7a-467c-9bae-ee8644dae6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: Uncomment and run this cell if you're running on your own device and want to use Docker instead.\n",
    "# !docker stop tensorflow-serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a63d3-e8c0-471b-bf1b-8b6dd368f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kill model server\n",
    "!kill $(ps aux | grep 'tensorflow_model_server' | awk '{print $2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
